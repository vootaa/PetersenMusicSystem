"""
PetersenéŸ³å¾‹ç³»ç»Ÿä¸»æ¢ç´¢æ§åˆ¶å™¨
åè°ƒæ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´çš„æ¢ç´¢å’Œåˆ†ææµç¨‹
"""
from typing import List, Dict, Tuple, Optional, Any, Callable
from dataclasses import dataclass
import time
import json
from pathlib import Path
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# æ·»åŠ çˆ¶çº§è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent.parent))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from .core.parameter_explorer import ParameterSpaceExplorer, ExplorationResult, format_exploration_result
from .core.characteristic_analyzer import CharacteristicAnalyzer
from .core.evaluation_framework import MultiDimensionalEvaluator, ComprehensiveEvaluation
from .core.classification_system import OpenClassificationSystem, ClassificationResult, format_classification_result
from .audio.playback_tester import PetersenPlaybackTester, SystemPlaybackAssessment, format_playback_assessment
from .reporting.report_generator import PetersenExplorationReportGenerator

@dataclass
class ExplorationConfiguration:
    """æ¢ç´¢é…ç½®"""
    # å‚æ•°ç©ºé—´é…ç½®
    f_base_candidates: List[float] = None
    f_min: float = 110.0
    f_max: float = 880.0
    
    # ç­›é€‰æ ‡å‡†
    min_entries: int = 5
    max_entries: int = 60
    min_interval_cents: float = 5.0
    max_interval_cents: float = 600.0
    
    # åˆ†æé…ç½®
    enable_audio_testing: bool = True
    enable_detailed_analysis: bool = True
    enable_reporting: bool = True
    
    # æ€§èƒ½é…ç½®
    max_workers: int = 4
    batch_size: int = 50
    
    # éŸ³é¢‘é…ç½®
    steinway_soundfont: str = "GD_Steinway_Model_D274.sf2"  # æˆ– "GD_Steinway_Model_D274II.sf2"
    audio_test_sample_size: int = 20  # éŸ³é¢‘æµ‹è¯•çš„ç³»ç»Ÿæ•°é‡
    
    # æŠ¥å‘Šé…ç½®
    report_name: str = None
    output_dir: Path = None

class PetersenMainExplorer:
    """PetersenéŸ³å¾‹ç³»ç»Ÿä¸»æ¢ç´¢å™¨"""
    
    def __init__(self, config: ExplorationConfiguration = None):
        """
        åˆå§‹åŒ–ä¸»æ¢ç´¢å™¨
        
        Args:
            config: æ¢ç´¢é…ç½®
        """
        self.config = config or ExplorationConfiguration()
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.parameter_explorer = ParameterSpaceExplorer(
            f_base_candidates=self.config.f_base_candidates,
            f_min=self.config.f_min,
            f_max=self.config.f_max
        )
        
        self.characteristic_analyzer = CharacteristicAnalyzer()
        self.evaluator = MultiDimensionalEvaluator()
        self.classifier = OpenClassificationSystem()
        
        if self.config.enable_reporting:
            self.report_generator = PetersenExplorationReportGenerator(
                output_dir=self.config.output_dir
            )
        
        # æ¢ç´¢çŠ¶æ€
        self.exploration_results: List[ExplorationResult] = []
        self.characteristics: Dict[str, Any] = {}
        self.evaluations: Dict[str, ComprehensiveEvaluation] = {}
        self.classifications: Dict[str, ClassificationResult] = {}
        self.audio_assessments: Dict[str, SystemPlaybackAssessment] = {}
        
        # è¿›åº¦å›è°ƒ
        self.progress_callbacks: List[Callable] = []
    
    def add_progress_callback(self, callback: Callable):
        """æ·»åŠ è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callbacks.append(callback)
    
    def run_complete_exploration(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ¢ç´¢æµç¨‹
        
        Returns:
            Dict: æ¢ç´¢ç»“æœæ‘˜è¦
        """
        print("ğŸš€ å¼€å§‹PetersenéŸ³å¾‹ç³»ç»Ÿå®Œæ•´æ¢ç´¢")
        print(f"ğŸ“Š é¢„è®¡æµ‹è¯•ç»„åˆæ•°: {self.parameter_explorer.total_combinations}")
        print(f"âš™ï¸ é…ç½®: {self._format_config()}")
        
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°ç©ºé—´æ¢ç´¢
            print("\n" + "="*60)
            print("ğŸ“¡ ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°ç©ºé—´ç³»ç»Ÿæ€§æ¢ç´¢")
            print("="*60)
            self._run_parameter_exploration()
            
            # ç­›é€‰æˆåŠŸçš„ç»“æœ
            successful_results = [r for r in self.exploration_results if r.success]
            print(f"\nâœ… å‚æ•°æ¢ç´¢å®Œæˆï¼š{len(successful_results)}/{len(self.exploration_results)} æˆåŠŸ")
            
            if not successful_results:
                print("âŒ æ²¡æœ‰æˆåŠŸçš„ç³»ç»Ÿï¼Œç»ˆæ­¢æ¢ç´¢")
                return {"status": "failed", "reason": "no_successful_systems"}
            
            # åº”ç”¨ç­›é€‰æ ‡å‡†
            filtered_results = self.parameter_explorer.filter_by_criteria(
                min_entries=self.config.min_entries,
                max_entries=self.config.max_entries,
                min_interval_cents=self.config.min_interval_cents,
                max_interval_cents=self.config.max_interval_cents
            )
            
            print(f"ğŸ“‹ ç­›é€‰åç³»ç»Ÿæ•°: {len(filtered_results)}")
            
            if not filtered_results:
                print("âš ï¸ ç­›é€‰åæ— å¯ç”¨ç³»ç»Ÿï¼Œæ”¾å®½ç­›é€‰æ ‡å‡†")
                filtered_results = successful_results[:50]  # å–å‰50ä¸ª
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç‰¹æ€§åˆ†æ
            if self.config.enable_detailed_analysis:
                print("\n" + "="*60)
                print("ğŸ”¬ ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç‰¹æ€§åˆ†æ")
                print("="*60)
                self._run_detailed_analysis(filtered_results)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šéŸ³é¢‘éªŒè¯æµ‹è¯•
            if self.config.enable_audio_testing:
                print("\n" + "="*60)
                print("ğŸµ ç¬¬ä¸‰é˜¶æ®µï¼šéŸ³é¢‘éªŒè¯æµ‹è¯•")
                print("="*60)
                self._run_audio_testing(filtered_results)
            
            # ç¬¬å››é˜¶æ®µï¼šæŠ¥å‘Šç”Ÿæˆ
            if self.config.enable_reporting:
                print("\n" + "="*60)
                print("ğŸ“‹ ç¬¬å››é˜¶æ®µï¼šç»¼åˆæŠ¥å‘Šç”Ÿæˆ")
                print("="*60)
                report_path = self._generate_comprehensive_report()
                print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
            # ç”Ÿæˆæ¢ç´¢æ‘˜è¦
            exploration_duration = time.time() - start_time
            summary = self._generate_exploration_summary(exploration_duration)
            
            print("\n" + "="*60)
            print("ğŸ‰ PetersenéŸ³å¾‹ç³»ç»Ÿæ¢ç´¢å®Œæˆ")
            print("="*60)
            print(f"â±ï¸ æ€»è€—æ—¶: {exploration_duration:.1f} ç§’")
            print(f"ğŸ“Š å¤„ç†ç³»ç»Ÿ: {len(self.exploration_results)}")
            print(f"âœ… æˆåŠŸç³»ç»Ÿ: {len(successful_results)}")
            print(f"ğŸ† ä¼˜ç§€ç³»ç»Ÿ: {len([e for e in self.evaluations.values() if e.weighted_total_score >= 0.7])}")
            print(f"ğŸµ éŸ³é¢‘æ¨è: {len([a for a in self.audio_assessments.values() if a.recommended_for_audio])}")
            
            return summary
            
        except Exception as e:
            print(f"\nâŒ æ¢ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(traceback.format_exc())
            return {
                "status": "error",
                "error": str(e),
                "duration": time.time() - start_time
            }
    
    def _run_parameter_exploration(self):
        """è¿è¡Œå‚æ•°ç©ºé—´æ¢ç´¢"""
        def progress_callback(current, total, result):
            percentage = current / total * 100
            status = "âœ…" if result.success else "âŒ"
            
            if current % 20 == 0 or current == total:  # æ¯20ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
                print(f"   [{current:4d}/{total}] ({percentage:5.1f}%) {status} {result.parameters.phi_name} Ã— {result.parameters.delta_theta_name}")
            
            # è°ƒç”¨å¤–éƒ¨å›è°ƒ
            for callback in self.progress_callbacks:
                callback("parameter_exploration", current, total, result)
        
        def error_callback(params, error):
            if "é¢‘ç‡èŒƒå›´" not in error:  # åªæ˜¾ç¤ºéå¸¸è§é”™è¯¯
                print(f"   âš ï¸ é”™è¯¯: {params.phi_name} Ã— {params.delta_theta_name} - {error}")
        
        self.exploration_results = self.parameter_explorer.explore_all_combinations(
            progress_callback=progress_callback,
            error_callback=error_callback
        )
    
    def _run_detailed_analysis(self, filtered_results: List[ExplorationResult]):
        """è¿è¡Œè¯¦åº¦åˆ†æ"""
        print(f"ğŸ”¬ åˆ†æ {len(filtered_results)} ä¸ªç­›é€‰åçš„ç³»ç»Ÿ...")
        
        def analyze_single_system(result: ExplorationResult) -> Tuple[str, Any, ComprehensiveEvaluation, ClassificationResult]:
            """åˆ†æå•ä¸ªç³»ç»Ÿ"""
            result_key = self._get_result_key(result)
            
            try:
                # ç‰¹æ€§åˆ†æ
                characteristics = self.characteristic_analyzer.analyze_scale_characteristics(
                    result.scale, result.entries
                )
                
                # å¤šç»´åº¦è¯„ä¼°
                evaluation = self.evaluator.evaluate_comprehensive(characteristics)
                
                # å¼€æ”¾æ€§åˆ†ç±»
                classification = self.classifier.classify_system(evaluation)
                
                return result_key, characteristics, evaluation, classification
                
            except Exception as e:
                print(f"   âŒ åˆ†æå¤±è´¥ {result_key}: {str(e)}")
                return result_key, None, None, None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œåˆ†æ
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤åˆ†æä»»åŠ¡
            futures = {
                executor.submit(analyze_single_system, result): result 
                for result in filtered_results
            }
            
            completed = 0
            for future in as_completed(futures):
                result_key, characteristics, evaluation, classification = future.result()
                
                if characteristics and evaluation and classification:
                    self.characteristics[result_key] = characteristics
                    self.evaluations[result_key] = evaluation
                    self.classifications[result_key] = classification
                
                completed += 1
                if completed % 10 == 0 or completed == len(filtered_results):
                    print(f"   ğŸ“Š å·²å®Œæˆåˆ†æ: {completed}/{len(filtered_results)} ({completed/len(filtered_results)*100:.1f}%)")
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                for callback in self.progress_callbacks:
                    callback("detailed_analysis", completed, len(filtered_results), None)
        
        print(f"âœ… è¯¦ç»†åˆ†æå®Œæˆï¼š{len(self.evaluations)} ä¸ªç³»ç»Ÿè·å¾—è¯„ä¼°ç»“æœ")
    
    def _run_audio_testing(self, filtered_results: List[ExplorationResult]):
        """è¿è¡ŒéŸ³é¢‘æµ‹è¯•"""
        # é€‰æ‹©å‰Nä¸ªæœ€ä¼˜ç³»ç»Ÿè¿›è¡ŒéŸ³é¢‘æµ‹è¯•
        if not self.evaluations:
            print("âš ï¸ æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œéšæœºé€‰æ‹©ç³»ç»Ÿè¿›è¡ŒéŸ³é¢‘æµ‹è¯•")
            test_systems = filtered_results[:self.config.audio_test_sample_size]
        else:
            # æŒ‰è¯„ä¼°å¾—åˆ†æ’åºé€‰æ‹©
            scored_systems = []
            for result in filtered_results:
                result_key = self._get_result_key(result)
                if result_key in self.evaluations:
                    score = self.evaluations[result_key].weighted_total_score
                    scored_systems.append((score, result))
            
            scored_systems.sort(key=lambda x: x[0], reverse=True)
            test_systems = [system[1] for system in scored_systems[:self.config.audio_test_sample_size]]
        
        print(f"ğŸµ æµ‹è¯• {len(test_systems)} ä¸ªä¼˜é€‰ç³»ç»Ÿçš„éŸ³é¢‘æ’­æ”¾èƒ½åŠ›...")
        
        try:
            with PetersenPlaybackTester(soundfont_path=self.config.steinway_soundfont) as tester:
                for i, result in enumerate(test_systems, 1):
                    result_key = self._get_result_key(result)
                    
                    print(f"\nğŸ¼ [{i}/{len(test_systems)}] æµ‹è¯•ç³»ç»Ÿ: {result_key}")
                    
                    try:
                        assessment = tester.test_system_playability(
                            result,
                            interactive=False  # éäº¤äº’æ¨¡å¼
                        )
                        self.audio_assessments[result_key] = assessment
                        
                        # è°ƒç”¨è¿›åº¦å›è°ƒ
                        for callback in self.progress_callbacks:
                            callback("audio_testing", i, len(test_systems), assessment)
                    
                    except Exception as e:
                        print(f"   âŒ éŸ³é¢‘æµ‹è¯•å¤±è´¥: {str(e)}")
        
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æµ‹è¯•æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print("   è·³è¿‡éŸ³é¢‘æµ‹è¯•é˜¶æ®µ")
        
        print(f"âœ… éŸ³é¢‘æµ‹è¯•å®Œæˆï¼š{len(self.audio_assessments)} ä¸ªç³»ç»Ÿè·å¾—æ’­æ”¾è¯„ä¼°")
    
    def _generate_comprehensive_report(self) -> Path:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆæ¢ç´¢æŠ¥å‘Š...")
        
        report_path = self.report_generator.generate_comprehensive_report(
            exploration_results=self.exploration_results,
            evaluations=self.evaluations,
            classifications=self.classifications,
            audio_assessments=self.audio_assessments,
            report_name=self.config.report_name
        )
        
        return report_path
    
    def _generate_exploration_summary(self, duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¢ç´¢æ‘˜è¦"""
        successful_results = [r for r in self.exploration_results if r.success]
        
        # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
        category_distribution = {}
        for classification in self.classifications.values():
            category = classification.primary_category.value
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        # è¯†åˆ«é¡¶çº§ç³»ç»Ÿ
        top_systems = []
        if self.evaluations:
            scored_systems = [
                (eval_result.weighted_total_score, result_key, eval_result)
                for result_key, eval_result in self.evaluations.items()
            ]
            scored_systems.sort(key=lambda x: x[0], reverse=True)
            
            for score, result_key, evaluation in scored_systems[:10]:
                # æ‰¾åˆ°å¯¹åº”çš„exploration_result
                matching_result = None
                for result in successful_results:
                    if self._get_result_key(result) == result_key:
                        matching_result = result
                        break
                
                if matching_result:
                    top_systems.append({
                        'result_key': result_key,
                        'score': score,
                        'parameters': matching_result.parameters,
                        'entry_count': len(matching_result.entries),
                        'category': self.classifications.get(result_key, {}).primary_category.value if result_key in self.classifications else 'unknown'
                    })
        
        # éŸ³é¢‘æ¨èç»Ÿè®¡
        audio_recommended_count = sum(1 for a in self.audio_assessments.values() if a.recommended_for_audio)
        
        return {
            "status": "completed",
            "duration": duration,
            "statistics": {
                "total_combinations": len(self.exploration_results),
                "successful_systems": len(successful_results),
                "failed_systems": len(self.exploration_results) - len(successful_results),
                "analyzed_systems": len(self.evaluations),
                "classified_systems": len(self.classifications),
                "audio_tested_systems": len(self.audio_assessments),
                "audio_recommended_systems": audio_recommended_count
            },
            "category_distribution": category_distribution,
            "top_systems": top_systems,
            "performance_metrics": {
                "avg_analysis_time": duration / len(self.evaluations) if self.evaluations else 0,
                "success_rate": len(successful_results) / len(self.exploration_results) if self.exploration_results else 0
            }
        }
    
    def get_top_systems(self, count: int = 10, 
                       criteria: str = "overall") -> List[Tuple[ExplorationResult, ComprehensiveEvaluation, ClassificationResult]]:
        """
        è·å–é¡¶çº§ç³»ç»Ÿ
        
        Args:
            count: è¿”å›æ•°é‡
            criteria: æ’åºæ ‡å‡† ("overall", "traditional", "experimental", "audio")
            
        Returns:
            List: é¡¶çº§ç³»ç»Ÿåˆ—è¡¨
        """
        if not self.evaluations:
            return []
        
        systems = []
        
        for result in self.exploration_results:
            if not result.success:
                continue
                
            result_key = self._get_result_key(result)
            
            if result_key in self.evaluations:
                evaluation = self.evaluations[result_key]
                classification = self.classifications.get(result_key)
                
                # æ ¹æ®æ ‡å‡†è®¡ç®—æ’åºåˆ†æ•°
                if criteria == "overall":
                    score = evaluation.weighted_total_score
                elif criteria == "traditional":
                    score = evaluation.dimension_scores['traditional_compatibility'].score
                elif criteria == "experimental":
                    score = evaluation.dimension_scores['experimental_innovation'].score
                elif criteria == "audio":
                    if result_key in self.audio_assessments:
                        score = self.audio_assessments[result_key].overall_playability
                    else:
                        score = 0
                else:
                    score = evaluation.weighted_total_score
                
                systems.append((score, result, evaluation, classification))
        
        # æŒ‰åˆ†æ•°æ’åº
        systems.sort(key=lambda x: x[0], reverse=True)
        
        return [(result, eval, classif) for score, result, eval, classif in systems[:count]]
    
    def export_top_systems_for_player(self, count: int = 5, output_dir: Path = None) -> List[Path]:
        """
        å¯¼å‡ºé¡¶çº§ç³»ç»Ÿä¸ºEnhanced Petersen Playerå¯ç”¨æ ¼å¼
        
        Args:
            count: å¯¼å‡ºæ•°é‡
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            List[Path]: å¯¼å‡ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        top_systems = self.get_top_systems(count)
        
        if not output_dir:
            output_dir = Path("./exported_scales")
        output_dir.mkdir(exist_ok=True)
        
        exported_files = []
        
        for i, (result, evaluation, classification) in enumerate(top_systems, 1):
            # ç”Ÿæˆæ–‡ä»¶å
            params = result.parameters
            filename = f"petersen_scale_{i:02d}_{params.phi_name}_{params.delta_theta_name}_{params.f_base}Hz.json"
            file_path = output_dir / filename
            
            # å‡†å¤‡å¯¼å‡ºæ•°æ®
            scale_data = {
                "name": f"Petersen Scale #{i}",
                "description": f"Ï†={params.phi_name}({params.phi_value:.6f}), Î´Î¸={params.delta_theta_name}({params.delta_theta_value}Â°)",
                "parameters": {
                    "phi_name": params.phi_name,
                    "phi_value": params.phi_value,
                    "delta_theta_name": params.delta_theta_name,
                    "delta_theta_value": params.delta_theta_value,
                    "f_base": params.f_base,
                    "f_min": params.f_min,
                    "f_max": params.f_max
                },
                "entries": [
                    {
                        "frequency": entry.freq,
                        "key_short": entry.key_short,
                        "key_long": entry.key_long,
                        "midi_note": getattr(entry, 'midi_note', None),
                        "cents_from_base": getattr(entry, 'cents_from_base', None)
                    }
                    for entry in result.entries
                ],
                "evaluation": {
                    "weighted_total_score": evaluation.weighted_total_score,
                    "category": classification.primary_category.value if classification else "unknown",
                    "strengths": evaluation.strengths,
                    "applications": evaluation.application_suggestions
                } if evaluation else None,
                "metadata": {
                    "generated_by": "PetersenExplorer",
                    "generation_time": time.time(),
                    "entry_count": len(result.entries)
                }
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(scale_data, f, indent=2, ensure_ascii=False)
            
            exported_files.append(file_path)
            print(f"   ğŸ“ å¯¼å‡º: {filename}")
        
        print(f"âœ… å·²å¯¼å‡º {len(exported_files)} ä¸ªé¡¶çº§éŸ³å¾‹ç³»ç»Ÿ")
        return exported_files
    
    def _get_result_key(self, result: ExplorationResult) -> str:
        """è·å–ç»“æœçš„å”¯ä¸€é”®"""
        params = result.parameters
        return f"{params.phi_name}_{params.delta_theta_name}_{params.f_base}"
    
    def _format_config(self) -> str:
        """æ ¼å¼åŒ–é…ç½®ä¿¡æ¯"""
        config_items = [
            f"F_baseå€™é€‰æ•°: {len(self.config.f_base_candidates or [])}" if self.config.f_base_candidates else "F_base: é»˜è®¤",
            f"é¢‘ç‡èŒƒå›´: {self.config.f_min}-{self.config.f_max}Hz",
            f"éŸ³ç¬¦ç­›é€‰: {self.config.min_entries}-{self.config.max_entries}ä¸ª",
            f"å¹¶è¡Œåº¦: {self.config.max_workers}çº¿ç¨‹"
        ]
        
        features = []
        if self.config.enable_detailed_analysis:
            features.append("è¯¦ç»†åˆ†æ")
        if self.config.enable_audio_testing:
            features.append("éŸ³é¢‘æµ‹è¯•")
        if self.config.enable_reporting:
            features.append("æŠ¥å‘Šç”Ÿæˆ")
        
        if features:
            config_items.append(f"åŠŸèƒ½: {', '.join(features)}")
        
        return " | ".join(config_items)

# ä¾¿æ·åŠŸèƒ½å‡½æ•°
def quick_exploration(f_base_list: List[float] = None, 
                     output_dir: Path = None,
                     enable_audio: bool = True) -> Dict[str, Any]:
    """
    å¿«é€Ÿæ¢ç´¢åŠŸèƒ½
    
    Args:
        f_base_list: F_baseå€™é€‰å€¼åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        enable_audio: æ˜¯å¦å¯ç”¨éŸ³é¢‘æµ‹è¯•
        
    Returns:
        Dict: æ¢ç´¢ç»“æœæ‘˜è¦
    """
    config = ExplorationConfiguration(
        f_base_candidates=f_base_list,
        enable_audio_testing=enable_audio,
        output_dir=output_dir,
        audio_test_sample_size=10 if enable_audio else 0
    )
    
    explorer = PetersenMainExplorer(config)
    return explorer.run_complete_exploration()

def explore_specific_presets(phi_names: List[str] = None,
                           delta_theta_names: List[str] = None,
                           f_base: float = 220.0) -> List[ExplorationResult]:
    """
    æ¢ç´¢ç‰¹å®šé¢„è®¾ç»„åˆ
    
    Args:
        phi_names: Ï†é¢„è®¾åç§°åˆ—è¡¨
        delta_theta_names: Î´Î¸é¢„è®¾åç§°åˆ—è¡¨  
        f_base: åŸºç¡€é¢‘ç‡
        
    Returns:
        List[ExplorationResult]: æ¢ç´¢ç»“æœ
    """
    from PetersenScale_Phi import PHI_PRESETS, DELTA_THETA_PRESETS
    
    # è¿‡æ»¤é¢„è®¾
    phi_presets = {k: v for k, v in PHI_PRESETS.items() if not phi_names or k in phi_names}
    dth_presets = {k: v for k, v in DELTA_THETA_PRESETS.items() if not delta_theta_names or k in delta_theta_names}
    
    print(f"ğŸ¯ æ¢ç´¢ç‰¹å®šé¢„è®¾ç»„åˆ:")
    print(f"   Ï†é¢„è®¾: {list(phi_presets.keys())}")
    print(f"   Î´Î¸é¢„è®¾: {list(dth_presets.keys())}")
    print(f"   F_base: {f_base}Hz")
    
    explorer = ParameterSpaceExplorer(f_base_candidates=[f_base])
    results = []
    
    for phi_name, phi_value in phi_presets.items():
        for dth_name, dth_value in dth_presets.items():
            from .core.parameter_explorer import ExplorationParameters
            
            params = ExplorationParameters(
                phi_name=phi_name,
                phi_value=phi_value,
                delta_theta_name=dth_name,
                delta_theta_value=dth_value,
                f_base=f_base
            )
            
            result = explorer.explore_single_combination(params)
            results.append(result)
            
            status = "âœ…" if result.success else "âŒ"
            print(f"   {status} {phi_name} Ã— {dth_name}: {len(result.entries) if result.success else 0} éŸ³ç¬¦")
    
    return results

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸ¼ PetersenéŸ³å¾‹ç³»ç»Ÿæ¢ç´¢å™¨ - æ¼”ç¤ºæ¨¡å¼")
    
    # å¿«é€Ÿæ¼”ç¤ºæ¢ç´¢
    print("\n1ï¸âƒ£ å¿«é€Ÿæ¢ç´¢æ¼”ç¤º (éƒ¨åˆ†å‚æ•°):")
    demo_config = ExplorationConfiguration(
        f_base_candidates=[220.0, 261.63],  # åªæµ‹è¯•A3å’ŒC4
        enable_audio_testing=False,  # æ¼”ç¤ºæ¨¡å¼å…³é—­éŸ³é¢‘æµ‹è¯•
        enable_detailed_analysis=True,
        audio_test_sample_size=5
    )
    
    explorer = PetersenMainExplorer(demo_config)
    
    # æ·»åŠ ç®€å•çš„è¿›åº¦æ˜¾ç¤º
    def simple_progress(stage, current, total, data):
        if current % 10 == 0 or current == total:
            print(f"   ğŸ“Š {stage}: {current}/{total} ({current/total*100:.1f}%)")
    
    explorer.add_progress_callback(simple_progress)
    
    try:
        summary = explorer.run_complete_exploration()
        
        print("\nğŸ“‹ æ¢ç´¢æ‘˜è¦:")
        print(f"   æ€»è®¡ç»„åˆ: {summary['statistics']['total_combinations']}")
        print(f"   æˆåŠŸç³»ç»Ÿ: {summary['statistics']['successful_systems']}")
        print(f"   åˆ†æç³»ç»Ÿ: {summary['statistics']['analyzed_systems']}")
        
        # æ˜¾ç¤ºå‰5åç³»ç»Ÿ
        print("\nğŸ† å‰5åç³»ç»Ÿ:")
        top_systems = explorer.get_top_systems(5)
        for i, (result, evaluation, classification) in enumerate(top_systems, 1):
            params = result.parameters
            print(f"   {i}. {params.phi_name} Ã— {params.delta_theta_name} (è¯„åˆ†: {evaluation.weighted_total_score:.3f})")
        
        # å¯¼å‡ºé¡¶çº§ç³»ç»Ÿ
        print("\nğŸ“ å¯¼å‡ºå‰3åç³»ç»Ÿ...")
        exported = explorer.export_top_systems_for_player(3)
        print(f"   å¯¼å‡ºå®Œæˆ: {len(exported)} ä¸ªæ–‡ä»¶")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ¢ç´¢")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()