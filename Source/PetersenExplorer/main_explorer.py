"""
PetersenéŸ³å¾‹ç³»ç»Ÿä¸»æ¢ç´¢æ§åˆ¶å™¨
åè°ƒæ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´çš„æ¢ç´¢å’Œåˆ†ææµç¨‹
"""
import time
import traceback
from typing import List, Dict, Tuple, Optional, Any, Callable
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

import sys
from pathlib import Path

# æ·»åŠ çˆ¶çº§è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent))
sys.path.insert(0, str(current_dir))

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ›´å¥½çš„å¯¼å…¥å¤„ç†
try:
    # é¦–å…ˆå°è¯•å¯¼å…¥PetersenScale_Phi
    from PetersenScale_Phi import PetersenScale_Phi, PHI_PRESETS, DELTA_THETA_PRESETS
    PETERSEN_SCALE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥PetersenScale_Phi: {e}")
    print("è¯·ç¡®ä¿PetersenScale_Phi.pyåœ¨æ­£ç¡®çš„è·¯å¾„ä¸­")
    sys.exit(1)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core.parameter_explorer import ParameterSpaceExplorer, ExplorationResult
    from core.characteristic_analyzer import CharacteristicAnalyzer  
    from core.evaluation_framework import MultiDimensionalEvaluator, ComprehensiveEvaluation
    from core.classification_system import OpenClassificationSystem, ClassificationResult
    from reporting.report_generator import PetersenExplorationReportGenerator
    CORE_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    CORE_MODULES_AVAILABLE = False
    
    # ç®€åŒ–ç±»å®šä¹‰
    class ExplorationResult:
        def __init__(self, parameters, scale, entries, success, basic_metrics=None):
            self.parameters = parameters
            self.scale = scale
            self.entries = entries
            self.success = success
            self.basic_metrics = basic_metrics or {}
    
    class ParameterSpaceExplorer:
        def __init__(self, f_base_candidates, f_min, f_max):
            self.f_base_candidates = f_base_candidates
            self.f_min = f_min
            self.f_max = f_max
            self.total_combinations = len(PHI_PRESETS) * len(DELTA_THETA_PRESETS) * len(f_base_candidates)
        
        def explore_all_combinations(self, progress_callback=None, error_callback=None):
            return self._simplified_exploration(progress_callback, error_callback)
        
        def _simplified_exploration(self, progress_callback, error_callback):
            # ç®€åŒ–çš„æ¢ç´¢å®ç°
            results = []
            count = 0
            
            for phi_name, phi_value in list(PHI_PRESETS.items())[:5]:  # é™åˆ¶æµ‹è¯•æ•°é‡
                for theta_name, theta_value in list(DELTA_THETA_PRESETS.items())[:5]:
                    for f_base in self.f_base_candidates:
                        count += 1
                        
                        try:
                            from types import SimpleNamespace
                            params = SimpleNamespace(
                                phi_name=phi_name,
                                phi_value=phi_value,
                                delta_theta_name=theta_name,
                                delta_theta_value=theta_value,
                                f_base=f_base
                            )
                            
                            scale = PetersenScale_Phi(
                                F_base=f_base,
                                delta_theta=theta_value,
                                phi=phi_value,
                                F_min=self.f_min,
                                F_max=self.f_max
                            )
                            
                            entries = scale.generate()
                            
                            result = ExplorationResult(
                                parameters=params,
                                scale=scale,
                                entries=entries,
                                success=True,
                                basic_metrics={
                                    'entry_count': len(entries),
                                    'frequency_range': (entries[0]['freq'], entries[-1]['freq']) if entries else (0, 0)
                                }
                            )
                            
                            results.append(result)
                            
                            if progress_callback:
                                progress_callback(count, min(25, self.total_combinations), result)
                            
                        except Exception as e:
                            if error_callback:
                                error_callback(params, str(e))
                            continue
            
            return results
        
        def filter_by_criteria(self, results=None, **kwargs):
            if not hasattr(self, '_last_results'):
                return []
            results = results or self._last_results
            return [r for r in results if r.success and len(r.entries) >= kwargs.get('min_entries', 5)]
    
    class CharacteristicAnalyzer:
        def analyze_scale_characteristics(self, scale, entries):
            return None
    
    class MultiDimensionalEvaluator:
        def evaluate_comprehensive(self, characteristics):
            return None
    
    class OpenClassificationSystem:
        def classify_system(self, evaluation):
            return None
    
    class PetersenExplorationReportGenerator:
        def __init__(self, **kwargs):
            pass
        
        def generate_comprehensive_report(self, **kwargs):
            return Path("./report.txt")

# æ¡ä»¶å¯¼å…¥éŸ³é¢‘æ¨¡å—
try:
    from audio.playback_tester import PetersenPlaybackTester, SystemPlaybackAssessment
    AUDIO_AVAILABLE = True
except ImportError:
    print("âš ï¸ éŸ³é¢‘æµ‹è¯•æ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡éŸ³é¢‘éªŒè¯")
    PetersenPlaybackTester = None
    SystemPlaybackAssessment = None
    AUDIO_AVAILABLE = False

@dataclass
class ExplorationConfiguration:
    """æ¢ç´¢é…ç½®"""
    # å‚æ•°ç©ºé—´é…ç½®
    f_base_candidates: List[float] = field(default_factory=lambda: [110.0, 220.0, 261.63])
    f_min: float = 110.0
    f_max: float = 880.0
    
    # ç­›é€‰æ ‡å‡†
    min_entries: int = 5
    max_entries: int = 60
    min_interval_cents: float = 5.0
    max_interval_cents: float = 600.0
    
    # åˆ†æé…ç½®
    enable_audio_testing: bool = False  # é»˜è®¤å…³é—­éŸ³é¢‘æµ‹è¯•
    enable_detailed_analysis: bool = True
    enable_reporting: bool = True
    
    # æ€§èƒ½é…ç½®
    max_workers: int = 2  # å‡å°‘å¹¶å‘æ•°
    batch_size: int = 50
    
    # éŸ³é¢‘é…ç½®
    steinway_soundfont: str = "GD_Steinway_Model_D274.sf2"  # æˆ– "GD_Steinway_Model_D274II.sf2"
    audio_test_sample_size: int = 5  # éŸ³é¢‘æµ‹è¯•çš„ç³»ç»Ÿæ•°é‡
    
    # æŠ¥å‘Šé…ç½®
    report_name: str = None
    output_dir: Path = None

class PetersenMainExplorer:
    """PetersenéŸ³å¾‹ç³»ç»Ÿä¸»æ¢ç´¢å™¨"""
    
    def __init__(self, config: ExplorationConfiguration = None):
        """åˆå§‹åŒ–ä¸»æ¢ç´¢å™¨"""
        self.config = config or ExplorationConfiguration()
        
        # åˆå§‹åŒ–å„æ¨¡å—
        try:
            self.parameter_explorer = ParameterSpaceExplorer(
                f_base_candidates=self.config.f_base_candidates,
                f_min=self.config.f_min,
                f_max=self.config.f_max
            )
            
            self.characteristic_analyzer = CharacteristicAnalyzer()
            self.evaluator = MultiDimensionalEvaluator()
            self.classifier = OpenClassificationSystem()
            
            if self.config.enable_reporting:
                self.report_generator = PetersenExplorationReportGenerator(
                    output_dir=self.config.output_dir
                )
        except Exception as e:
            print(f"âš ï¸ æ¨¡å—åˆå§‹åŒ–è­¦å‘Š: {e}")
            print("å°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½æ¨¡å¼")
        
        # æ¢ç´¢çŠ¶æ€
        self.exploration_results: List[ExplorationResult] = []
        self.characteristics: Dict[str, Any] = {}
        self.evaluations: Dict[str, ComprehensiveEvaluation] = {}
        self.classifications: Dict[str, ClassificationResult] = {}
        self.audio_assessments: Dict[str, Any] = {}
        
        # è¿›åº¦å›è°ƒ
        self.progress_callbacks: List[Callable] = []
    
    def add_progress_callback(self, callback: Callable):
        """æ·»åŠ è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callbacks.append(callback)
    
    def run_complete_exploration(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ¢ç´¢æµç¨‹"""
        print("ğŸš€ å¼€å§‹PetersenéŸ³å¾‹ç³»ç»Ÿå®Œæ•´æ¢ç´¢")
        print(f"ğŸ“Š é¢„è®¡æµ‹è¯•ç»„åˆæ•°: {self.parameter_explorer.total_combinations}")
        print(f"âš™ï¸ é…ç½®: {self._format_config()}")
        
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°ç©ºé—´æ¢ç´¢
            print("\n" + "="*60)
            print("ğŸ“¡ ç¬¬ä¸€é˜¶æ®µï¼šå‚æ•°ç©ºé—´ç³»ç»Ÿæ€§æ¢ç´¢")
            print("="*60)
            self._run_parameter_exploration()
            
            # ç­›é€‰æˆåŠŸçš„ç»“æœ
            successful_results = [r for r in self.exploration_results if r.success]
            print(f"\nâœ… å‚æ•°æ¢ç´¢å®Œæˆï¼š{len(successful_results)}/{len(self.exploration_results)} æˆåŠŸ")
            
            if not successful_results:
                return {"status": "no_valid_systems", "duration": time.time() - start_time}
            
            # åº”ç”¨ç­›é€‰æ ‡å‡†
            filtered_results = self.parameter_explorer.filter_by_criteria(
                min_entries=self.config.min_entries,
                max_entries=self.config.max_entries,
                min_interval_cents=self.config.min_interval_cents,
                max_interval_cents=self.config.max_interval_cents
            )
            
            print(f"ğŸ“‹ ç­›é€‰åç³»ç»Ÿæ•°: {len(filtered_results)}")
            
            if not filtered_results:
                return {"status": "no_systems_pass_filter", "duration": time.time() - start_time}
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç‰¹æ€§åˆ†æ
            if self.config.enable_detailed_analysis:
                print("\n" + "="*60)
                print("ğŸ”¬ ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç‰¹æ€§åˆ†æ")
                print("="*60)
                self._run_detailed_analysis(filtered_results)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šéŸ³é¢‘éªŒè¯æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
            if self.config.enable_audio_testing and AUDIO_AVAILABLE:
                print("\n" + "="*60)
                print("ğŸµ ç¬¬ä¸‰é˜¶æ®µï¼šéŸ³é¢‘éªŒè¯æµ‹è¯•")
                print("="*60)
                self._run_audio_testing(filtered_results)
            
            # ç¬¬å››é˜¶æ®µï¼šæŠ¥å‘Šç”Ÿæˆ
            if self.config.enable_reporting:
                print("\n" + "="*60)
                print("ğŸ“‹ ç¬¬å››é˜¶æ®µï¼šæŠ¥å‘Šç”Ÿæˆ")
                print("="*60)
                self._generate_comprehensive_report()
            
            # ç”Ÿæˆæ¢ç´¢æ‘˜è¦
            exploration_duration = time.time() - start_time
            summary = self._generate_exploration_summary(exploration_duration)
            
            print("\n" + "="*60)
            print("ğŸ‰ PetersenéŸ³å¾‹ç³»ç»Ÿæ¢ç´¢å®Œæˆ")
            print("="*60)
            print(f"â±ï¸ æ€»è€—æ—¶: {exploration_duration:.1f} ç§’")
            print(f"ğŸ“Š å¤„ç†ç³»ç»Ÿ: {len(self.exploration_results)}")
            print(f"âœ… æˆåŠŸç³»ç»Ÿ: {len(successful_results)}")
            print(f"ğŸ† ä¼˜ç§€ç³»ç»Ÿ: {len([e for e in self.evaluations.values() if e.weighted_total_score >= 0.7])}")
            
            return summary
            
        except Exception as e:
            print(f"\nâŒ æ¢ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(traceback.format_exc())
            return {
                "status": "error",
                "error": str(e),
                "duration": time.time() - start_time
            }
    
    def _run_parameter_exploration(self):
        """è¿è¡Œå‚æ•°ç©ºé—´æ¢ç´¢"""
        def progress_callback(current, total, result):
            percentage = current / total * 100
            status = "âœ…" if result.success else "âŒ"
            
            if current % 5 == 0 or current == total:
                print(f"  ğŸ“Š è¿›åº¦: {current}/{total} ({percentage:.1f}%) {status}")
        
        def error_callback(params, error):
            if len(error) < 50:
                print(f"âš ï¸ ç”Ÿæˆå¤±è´¥: {params.phi_name}Ã—{params.delta_theta_name} - {error}")
        
        if CORE_MODULES_AVAILABLE:
            self.exploration_results = self.parameter_explorer.explore_all_combinations(
                progress_callback=progress_callback,
                error_callback=error_callback
            )
        else:
            self.exploration_results = self.parameter_explorer._simplified_exploration(
                progress_callback, error_callback
            )
            self.parameter_explorer._last_results = self.exploration_results

    def _run_simple_exploration(self):
        """ç®€åŒ–çš„æ¢ç´¢æ¨¡å¼ï¼Œç”¨äºæµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        print("ğŸ”§ è¿è¡Œç®€åŒ–æ¢ç´¢æ¨¡å¼...")
        
        # åˆ›å»ºä¸€äº›ç¤ºä¾‹ç»“æœç”¨äºæµ‹è¯•
        from PetersenScale_Phi import PetersenScale_Phi, PHI_PRESETS, DELTA_THETA_PRESETS
        
        sample_results = []
        phi_names = list(PHI_PRESETS.keys())[:3]  # åªæµ‹è¯•å‰3ä¸ªÏ†å€¼
        delta_theta_names = list(DELTA_THETA_PRESETS.keys())[:3]  # åªæµ‹è¯•å‰3ä¸ªÎ´Î¸å€¼
        
        for phi_name in phi_names:
            for delta_theta_name in delta_theta_names:
                try:
                    phi_value = PHI_PRESETS[phi_name]['value']
                    delta_theta_value = DELTA_THETA_PRESETS[delta_theta_name]['value']
                    
                    scale = PetersenScale_Phi(
                        F_base=220.0,
                        delta_theta=delta_theta_value,
                        phi=phi_value,
                        F_min=self.config.f_min,
                        F_max=self.config.f_max
                    )
                    
                    entries = scale.generate()
                    
                    # åˆ›å»ºç®€å•çš„ç»“æœå¯¹è±¡
                    result = type('ExplorationResult', (), {
                        'success': True,
                        'parameters': type('Parameters', (), {
                            'phi_name': phi_name,
                            'delta_theta_name': delta_theta_name,
                            'f_base': 220.0
                        })(),
                        'entries': entries,
                        'scale': scale,
                        'basic_metrics': {
                            'entry_count': len(entries),
                            'frequency_range': (entries[0]['frequency'], entries[-1]['frequency']) if entries else (0, 0)
                        }
                    })()
                    
                    sample_results.append(result)
                    print(f"  âœ… {phi_name} Ã— {delta_theta_name}: {len(entries)} éŸ³ç¬¦")
                    
                except Exception as e:
                    print(f"  âŒ {phi_name} Ã— {delta_theta_name}: {str(e)}")
        
        self.exploration_results = sample_results
        print(f"ğŸ“Š ç®€åŒ–æ¢ç´¢å®Œæˆï¼šç”Ÿæˆ {len(sample_results)} ä¸ªæµ‹è¯•ç³»ç»Ÿ")
    
    def _apply_filters(self, results):
        """åº”ç”¨ç­›é€‰æ¡ä»¶"""
        filtered = []
        for result in results:
            if hasattr(result, 'entries') and result.entries:
                entry_count = len(result.entries)
                if self.config.min_entries <= entry_count <= self.config.max_entries:
                    filtered.append(result)
        return filtered
    
    def _run_detailed_analysis(self, filtered_results: List[ExplorationResult]):
        """è¿è¡Œè¯¦ç»†åˆ†æ"""
        print(f"ğŸ”¬ åˆ†æ {len(filtered_results)} ä¸ªç­›é€‰åçš„ç³»ç»Ÿ...")
        
        def analyze_single_system(result: ExplorationResult) -> Tuple[str, Any, ComprehensiveEvaluation, ClassificationResult]:
            """åˆ†æå•ä¸ªç³»ç»Ÿ"""
            result_key = self._get_result_key(result)
            
            try:
                # ç‰¹æ€§åˆ†æ
                characteristics = self.characteristic_analyzer.analyze_scale_characteristics(
                    result.scale, result.entries
                )
                
                # å¤šç»´åº¦è¯„ä¼°
                evaluation = self.evaluator.evaluate_comprehensive(characteristics)
                
                # å¼€æ”¾æ€§åˆ†ç±»
                classification = self.classifier.classify_system(evaluation)
                
                return result_key, characteristics, evaluation, classification
                
            except Exception as e:
                print(f"âŒ åˆ†æç³»ç»Ÿå¤±è´¥ {result_key}: {e}")
                return result_key, None, None, None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œåˆ†æ
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤åˆ†æä»»åŠ¡
            futures = {
                executor.submit(analyze_single_system, result): result 
                for result in filtered_results
            }
            
            completed = 0
            for future in as_completed(futures):
                result_key, characteristics, evaluation, classification = future.result()
                
                if characteristics and evaluation and classification:
                    self.characteristics[result_key] = characteristics
                    self.evaluations[result_key] = evaluation
                    self.classifications[result_key] = classification
                
                completed += 1
                if completed % 5 == 0:
                    print(f"  ğŸ“Š åˆ†æè¿›åº¦: {completed}/{len(filtered_results)}")
        
        print(f"âœ… è¯¦ç»†åˆ†æå®Œæˆï¼š{len(self.evaluations)} ä¸ªç³»ç»Ÿè·å¾—è¯„ä¼°ç»“æœ")
    
    def _run_audio_testing(self, filtered_results: List[ExplorationResult]):
        """è¿è¡ŒéŸ³é¢‘æµ‹è¯•"""
        # é€‰æ‹©å‰Nä¸ªæœ€ä¼˜ç³»ç»Ÿè¿›è¡ŒéŸ³é¢‘æµ‹è¯•
        if not self.evaluations:
            print("âš ï¸ æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œéšæœºé€‰æ‹©ç³»ç»Ÿè¿›è¡ŒéŸ³é¢‘æµ‹è¯•")
            test_systems = filtered_results[:self.config.audio_test_sample_size]
        else:
            # æŒ‰è¯„ä¼°å¾—åˆ†æ’åºé€‰æ‹©
            scored_systems = []
            for result in filtered_results:
                result_key = self._get_result_key(result)
                if result_key in self.evaluations:
                    evaluation = self.evaluations[result_key]
                    scored_systems.append((evaluation.weighted_total_score, result))
            
            scored_systems.sort(key=lambda x: x[0], reverse=True)
            test_systems = [system[1] for system in scored_systems[:self.config.audio_test_sample_size]]
        
        print(f"ğŸµ æµ‹è¯• {len(test_systems)} ä¸ªä¼˜é€‰ç³»ç»Ÿçš„éŸ³é¢‘æ’­æ”¾èƒ½åŠ›...")
        
        try:
            with PetersenPlaybackTester(soundfont_path=self.config.steinway_soundfont) as tester:
                for i, result in enumerate(test_systems, 1):
                    result_key = self._get_result_key(result)
                    print(f"  ğŸ¼ [{i}/{len(test_systems)}] æµ‹è¯• {result_key}")
                    
                    assessment = tester.test_system_playability(result, interactive=False)
                    self.audio_assessments[result_key] = assessment
        
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æµ‹è¯•æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print("   è·³è¿‡éŸ³é¢‘æµ‹è¯•é˜¶æ®µ")
        
        print(f"âœ… éŸ³é¢‘æµ‹è¯•å®Œæˆï¼š{len(self.audio_assessments)} ä¸ªç³»ç»Ÿè·å¾—æ’­æ”¾è¯„ä¼°")
    
    def _generate_comprehensive_report(self) -> Path:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆæ¢ç´¢æŠ¥å‘Š...")
        
        report_path = self.report_generator.generate_comprehensive_report(
            exploration_results=self.exploration_results,
            evaluations=self.evaluations,
            classifications=self.classifications,
            audio_assessments=self.audio_assessments,
            report_name=self.config.report_name
        )
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path
    
    def _generate_exploration_summary(self, duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¢ç´¢æ‘˜è¦"""
        successful_results = [r for r in self.exploration_results if r.success]
        
        # ç»Ÿè®¡åˆ†ç±»åˆ†å¸ƒ
        category_distribution = {}
        for classification in self.classifications.values():
            category = classification.primary_category.value
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        # è¯†åˆ«é¡¶çº§ç³»ç»Ÿ
        top_systems = []
        if self.evaluations:
            scored_systems = [
                (eval_result.weighted_total_score, result_key, eval_result)
                for result_key, eval_result in self.evaluations.items()
            ]
            scored_systems.sort(key=lambda x: x[0], reverse=True)
            
            for score, result_key, evaluation in scored_systems[:10]:
                params_parts = result_key.split('_')
                top_systems.append({
                    'result_key': result_key,
                    'phi_name': params_parts[0] if len(params_parts) > 0 else "unknown",
                    'delta_theta_name': params_parts[1] if len(params_parts) > 1 else "unknown",
                    'f_base': params_parts[2] if len(params_parts) > 2 else "unknown",
                    'score': score
                })
        
        # éŸ³é¢‘æ¨èç»Ÿè®¡
        audio_recommended_count = sum(1 for a in self.audio_assessments.values() 
                                    if hasattr(a, 'recommended_for_audio') and a.recommended_for_audio)
        
        return {
            "status": "completed",
            "duration": duration,
            "statistics": {
                "total_combinations": len(self.exploration_results),
                "successful_systems": len(successful_results),
                "failed_systems": len(self.exploration_results) - len(successful_results),
                "analyzed_systems": len(self.evaluations),
                "classified_systems": len(self.classifications),
                "audio_tested_systems": len(self.audio_assessments),
                "audio_recommended_systems": audio_recommended_count
            },
            "category_distribution": category_distribution,
            "top_systems": top_systems,
            "performance_metrics": {
                "avg_analysis_time": duration / len(self.evaluations) if self.evaluations else 0,
                "success_rate": len(successful_results) / len(self.exploration_results) if self.exploration_results else 0
            }
        }
    
    def get_top_systems(self, count: int = 10, 
                       criteria: str = "overall") -> List[Tuple[ExplorationResult, ComprehensiveEvaluation, ClassificationResult]]:
        """è·å–é¡¶çº§ç³»ç»Ÿ"""
        if not self.evaluations:
            return []
        
        systems = []
        
        for result in self.exploration_results:
            if not result.success:
                continue
                
            result_key = self._get_result_key(result)
            
            if result_key in self.evaluations:
                eval_result = self.evaluations[result_key]
                classif_result = self.classifications.get(result_key)
                
                if criteria == "traditional":
                    score = eval_result.dimension_scores['traditional_compatibility'].score
                elif criteria == "experimental":
                    score = eval_result.dimension_scores['experimental_innovation'].score
                elif criteria == "audio" and result_key in self.audio_assessments:
                    assessment = self.audio_assessments[result_key]
                    score = getattr(assessment, 'overall_playability', 0)
                else:
                    score = eval_result.weighted_total_score
                
                systems.append((score, result, eval_result, classif_result))
        
        # æŒ‰åˆ†æ•°æ’åº
        systems.sort(key=lambda x: x[0], reverse=True)
        
        return [(result, eval, classif) for score, result, eval, classif in systems[:count]]
    
    def _get_result_key(self, result: ExplorationResult) -> str:
        """è·å–ç»“æœçš„å”¯ä¸€é”®"""
        params = result.parameters
        return f"{params.phi_name}_{params.delta_theta_name}_{params.f_base}"
    
    def _format_config(self) -> str:
        """æ ¼å¼åŒ–é…ç½®ä¿¡æ¯"""
        config_items = [
            f"F_baseå€™é€‰æ•°: {len(self.config.f_base_candidates)}",
            f"é¢‘ç‡èŒƒå›´: {self.config.f_min}-{self.config.f_max}Hz",
            f"éŸ³ç¬¦ç­›é€‰: {self.config.min_entries}-{self.config.max_entries}ä¸ª",
            f"å¹¶è¡Œåº¦: {self.config.max_workers}çº¿ç¨‹"
        ]
        
        features = []
        if self.config.enable_detailed_analysis:
            features.append("è¯¦ç»†åˆ†æ")
        if self.config.enable_audio_testing:
            features.append("éŸ³é¢‘æµ‹è¯•")
        if self.config.enable_reporting:
            features.append("æŠ¥å‘Šç”Ÿæˆ")
        
        if features:
            config_items.append(f"åŠŸèƒ½: {', '.join(features)}")
        
        return " | ".join(config_items)

# ä¾¿æ·åŠŸèƒ½å‡½æ•°
def quick_exploration(f_base_list: List[float] = None, 
                     output_dir: Path = None,
                     enable_audio: bool = False) -> Dict[str, Any]:
    """å¿«é€Ÿæ¢ç´¢åŠŸèƒ½"""
    config = ExplorationConfiguration(
        f_base_candidates=f_base_list or [220.0, 261.63],
        enable_audio_testing=enable_audio,
        output_dir=output_dir,
        audio_test_sample_size=3 if enable_audio else 0
    )
    
    explorer = PetersenMainExplorer(config)
    return explorer.run_complete_exploration()

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸ¼ PetersenéŸ³å¾‹ç³»ç»Ÿæ¢ç´¢å™¨ - æ¼”ç¤ºæ¨¡å¼")
    
    # å¿«é€Ÿæ¼”ç¤ºæ¢ç´¢
    print("\n1ï¸âƒ£ å¿«é€Ÿæ¢ç´¢æ¼”ç¤º:")
    demo_config = ExplorationConfiguration(
        f_base_candidates=[220.0],  # åªæµ‹è¯•A3
        enable_audio_testing=False,
        enable_detailed_analysis=True,
        audio_test_sample_size=0
    )
    
    explorer = PetersenMainExplorer(demo_config)
    
    try:
        summary = explorer.run_complete_exploration()
        
        print("\nğŸ“‹ æ¢ç´¢æ‘˜è¦:")
        print(f"   æ€»è®¡ç»„åˆ: {summary['statistics']['total_combinations']}")
        print(f"   æˆåŠŸç³»ç»Ÿ: {summary['statistics']['successful_systems']}")
        print(f"   åˆ†æç³»ç»Ÿ: {summary['statistics']['analyzed_systems']}")
        
        # æ˜¾ç¤ºå‰5åç³»ç»Ÿ
        print("\nğŸ† å‰5åç³»ç»Ÿ:")
        top_systems = explorer.get_top_systems(5)
        for i, (result, evaluation, classification) in enumerate(top_systems, 1):
            params = result.parameters
            score = getattr(evaluation, 'weighted_total_score', 0) if evaluation else 0
            category = getattr(classification.primary_category, 'value', 'æœªçŸ¥') if classification else "æœªåˆ†ç±»"
            print(f"   {i}. {params.phi_name} Ã— {params.delta_theta_name} "
                  f"(è¯„åˆ†: {score:.3f}, ç±»åˆ«: {category})")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ¢ç´¢")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
        traceback.print_exc()