"""
PetersenéŸ³å¾‹æ¢ç´¢æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
"""
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
import json
import csv
from pathlib import Path
from datetime import datetime
import sys

# æ·»åŠ çˆ¶çº§è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent.parent))

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..core.parameter_explorer import ExplorationResult
    from ..core.evaluation_framework import ComprehensiveEvaluation
    from ..core.classification_system import ClassificationResult
    from ..audio.playback_tester import SystemPlaybackAssessment
except ImportError:
    # å›é€€åˆ°ç»å¯¹å¯¼å…¥
    try:
        from core.parameter_explorer import ExplorationResult
        from core.evaluation_framework import ComprehensiveEvaluation
        from core.classification_system import ClassificationResult
        from audio.playback_tester import SystemPlaybackAssessment
    except ImportError:
        # åˆ›å»ºç®€åŒ–çš„æ›¿ä»£ç±»
        from typing import NamedTuple
        
        class ExplorationResult(NamedTuple):
            parameters: Any
            scale: Any
            entries: List
            success: bool
            basic_metrics: Dict
        
        class ComprehensiveEvaluation(NamedTuple):
            dimension_scores: Dict
            weighted_total_score: float
            category_recommendation: str
            application_suggestions: List
            strengths: List
            limitations: List
            overall_viability: str
        
        class ClassificationResult(NamedTuple):
            primary_category: Any
            confidence_score: float
        
        class SystemPlaybackAssessment(NamedTuple):
            overall_playability: float
            technical_score: float
            musical_score: float
            recommended_for_audio: bool

@dataclass
class ExplorationSummary:
    """æ¢ç´¢æ€»ç»“"""
    total_combinations_tested: int
    successful_systems: int
    failed_systems: int
    exploration_duration: float
    top_systems: List[Dict[str, Any]]
    category_distribution: Dict[str, int]
    recommendation_summary: Dict[str, Any]

class PetersenExplorationReportGenerator:
    """Petersenæ¢ç´¢æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: Path = None):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.output_dir = output_dir or Path("./reports")
        self.output_path = self.output_dir
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "detailed").mkdir(exist_ok=True)
        (self.output_dir / "summaries").mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        (self.output_dir / "audio_tests").mkdir(exist_ok=True)
    
    def generate_comprehensive_report(self,
                                    exploration_results: List[ExplorationResult],
                                    evaluations: Dict[str, ComprehensiveEvaluation] = None,
                                    classifications: Dict[str, ClassificationResult] = None,
                                    audio_assessments: Dict[str, SystemPlaybackAssessment] = None,
                                    report_name: str = None) -> Path:
        """
        ç”Ÿæˆç»¼åˆæ¢ç´¢æŠ¥å‘Š
        
        Args:
            exploration_results: æ¢ç´¢ç»“æœåˆ—è¡¨
            evaluations: è¯„ä¼°ç»“æœå­—å…¸
            classifications: åˆ†ç±»ç»“æœå­—å…¸
            audio_assessments: éŸ³é¢‘è¯„ä¼°ç»“æœå­—å…¸
            report_name: æŠ¥å‘Šåç§°
            
        Returns:
            Path: ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if not report_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_name = f"petersen_exploration_{timestamp}"
        
        print(f"ğŸ“‹ ç”Ÿæˆç»¼åˆæ¢ç´¢æŠ¥å‘Š: {report_name}")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = self.output_dir / report_name
        report_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå„éƒ¨åˆ†æŠ¥å‘Š
        self._generate_executive_summary(exploration_results, evaluations, 
                                       classifications, audio_assessments, report_dir)
        
        self._generate_detailed_analysis(exploration_results, evaluations,
                                       classifications, audio_assessments, report_dir)
        
        self._generate_data_exports(exploration_results, evaluations,
                                  classifications, audio_assessments, report_dir)
        
        self._generate_recommendations(exploration_results, evaluations,
                                     classifications, audio_assessments, report_dir)
        
        # ç”Ÿæˆä¸»æŠ¥å‘Šç´¢å¼•
        main_report_path = self._generate_main_report_index(report_dir, report_name)
        
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {main_report_path}")
        return main_report_path
    
    def _generate_executive_summary(self, exploration_results, evaluations, classifications, audio_assessments):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        with open(self.output_path / "executive_summary.md", "w", encoding="utf-8") as f:
            f.write("# PetersenExplorer æ¢ç´¢æ‰§è¡Œæ‘˜è¦\n\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            successful_results = [r for r in exploration_results if r.success]
            total_count = len(exploration_results)
            success_count = len(successful_results)
            
            f.write("## ğŸ“Š åŸºæœ¬ç»Ÿè®¡\n\n")
            
            # ä¿®å¤æ ¼å¼åŒ–é—®é¢˜
            if total_count > 0:
                success_rate = (success_count / total_count) * 100
                f.write(f"- **æ€»æµ‹è¯•ç»„åˆ**: {total_count}\n")
                f.write(f"- **æˆåŠŸç”Ÿæˆ**: {success_count} ({success_rate:.1f}%)\n")
            else:
                f.write(f"- **æ€»æµ‹è¯•ç»„åˆ**: {total_count}\n")
                f.write(f"- **æˆåŠŸç”Ÿæˆ**: {success_count} (0.0%)\n")
            
            f.write(f"- **è¯¦ç»†åˆ†æ**: {len(evaluations)}\n")
            f.write(f"- **ç³»ç»Ÿåˆ†ç±»**: {len(classifications)}\n")
            f.write(f"- **éŸ³é¢‘æµ‹è¯•**: {len(audio_assessments)}\n\n")
            
            # åˆ†ç±»åˆ†å¸ƒ
            if classifications:
                f.write("## ğŸ·ï¸ ç³»ç»Ÿåˆ†ç±»åˆ†å¸ƒ\n\n")
                category_counts = {}
                for classification in classifications.values():
                    category = classification.category if hasattr(classification, 'category') else "æœªåˆ†ç±»"
                    category_counts[category] = category_counts.get(category, 0) + 1
                
                for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- **{category}**: {count} ä¸ªç³»ç»Ÿ\n")
                f.write("\n")
            
            # è¯„ä¼°ç»“æœæ‘˜è¦
            if evaluations:
                f.write("## ğŸ¯ è¯„ä¼°ç»“æœæ‘˜è¦\n\n")
                scores = [eval_result.weighted_total_score if hasattr(eval_result, 'weighted_total_score') else 0 
                        for eval_result in evaluations.values()]
                
                if scores:
                    f.write(f"- **å¹³å‡è¯„åˆ†**: {sum(scores)/len(scores):.3f}\n")
                    f.write(f"- **æœ€é«˜è¯„åˆ†**: {max(scores):.3f}\n")
                    f.write(f"- **æœ€ä½è¯„åˆ†**: {min(scores):.3f}\n")
            
            f.write("\n---\n")
            f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    def _generate_detailed_analysis(self, exploration_results: List[ExplorationResult],
                                  evaluations: Dict, classifications: Dict,
                                  audio_assessments: Dict, report_dir: Path):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        analysis_dir = report_dir / "detailed_analysis"
        analysis_dir.mkdir(exist_ok=True)
        
        # Ï†å€¼åˆ†æ
        self._analyze_phi_presets(exploration_results, evaluations, analysis_dir)
        
        # Î´Î¸å€¼åˆ†æ
        self._analyze_delta_theta_presets(exploration_results, evaluations, analysis_dir)
        
        # F_baseåˆ†æ
        self._analyze_f_base_effects(exploration_results, evaluations, analysis_dir)
        
        # éŸ³ä¹ç‰¹æ€§åˆ†æ
        if evaluations:
            self._analyze_musical_characteristics(evaluations, analysis_dir)
        
        # éŸ³é¢‘æµ‹è¯•åˆ†æ
        if audio_assessments:
            self._analyze_audio_performance(audio_assessments, analysis_dir)
    
    def _analyze_phi_presets(self, exploration_results: List[ExplorationResult],
                           evaluations: Dict, analysis_dir: Path):
        """åˆ†æÏ†é¢„è®¾çš„å½±å“"""
        phi_analysis_path = analysis_dir / "phi_presets_analysis.md"
        
        # æŒ‰Ï†å€¼åˆ†ç»„
        phi_groups = {}
        for result in exploration_results:
            if result.success:
                phi_name = result.parameters.phi_name
                if phi_name not in phi_groups:
                    phi_groups[phi_name] = []
                phi_groups[phi_name].append(result)
        
        with open(phi_analysis_path, 'w', encoding='utf-8') as f:
            f.write("# Ï†å€¼é¢„è®¾åˆ†æ\n\n")
            
            for phi_name, results in sorted(phi_groups.items()):
                f.write(f"## Ï† = {phi_name} ({results[0].parameters.phi_value:.6f})\n\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                f.write(f"- **æˆåŠŸç»„åˆæ•°**: {len(results)}\n")
                
                # éŸ³ç¬¦æ•°é‡ç»Ÿè®¡
                entry_counts = [len(r.entries) for r in results]
                if entry_counts:
                    f.write(f"- **éŸ³ç¬¦æ•°é‡èŒƒå›´**: {min(entry_counts)} - {max(entry_counts)}\n")
                    f.write(f"- **å¹³å‡éŸ³ç¬¦æ•°**: {sum(entry_counts)/len(entry_counts):.1f}\n")
                
                # éŸ³ç¨‹ç‰¹æ€§
                if results[0].basic_metrics and 'avg_interval_cents' in results[0].basic_metrics:
                    avg_intervals = [r.basic_metrics['avg_interval_cents'] for r in results 
                                   if r.basic_metrics and 'avg_interval_cents' in r.basic_metrics]
                    if avg_intervals:
                        f.write(f"- **å¹³å‡éŸ³ç¨‹**: {sum(avg_intervals)/len(avg_intervals):.1f} éŸ³åˆ†\n")
                
                # è¯„ä¼°å¾—åˆ†ç»Ÿè®¡
                if evaluations:
                    eval_scores = []
                    for result in results:
                        result_key = self._get_result_key(result)
                        if result_key in evaluations:
                            eval_scores.append(evaluations[result_key].weighted_total_score)
                    
                    if eval_scores:
                        f.write(f"- **å¹³å‡è¯„ä¼°å¾—åˆ†**: {sum(eval_scores)/len(eval_scores):.3f}\n")
                        f.write(f"- **æœ€é«˜è¯„ä¼°å¾—åˆ†**: {max(eval_scores):.3f}\n")
                
                f.write("\n")
    
    def _analyze_delta_theta_presets(self, exploration_results: List[ExplorationResult],
                                   evaluations: Dict, analysis_dir: Path):
        """åˆ†æÎ´Î¸é¢„è®¾çš„å½±å“"""
        dth_analysis_path = analysis_dir / "delta_theta_presets_analysis.md"
        
        # æŒ‰Î´Î¸å€¼åˆ†ç»„
        dth_groups = {}
        for result in exploration_results:
            if result.success:
                dth_name = result.parameters.delta_theta_name
                if dth_name not in dth_groups:
                    dth_groups[dth_name] = []
                dth_groups[dth_name].append(result)
        
        with open(dth_analysis_path, 'w', encoding='utf-8') as f:
            f.write("# Î´Î¸å€¼é¢„è®¾åˆ†æ\n\n")
            
            for dth_name, results in sorted(dth_groups.items()):
                f.write(f"## Î´Î¸ = {dth_name} ({results[0].parameters.delta_theta_value}Â°)\n\n")
                
                # ç±»ä¼¼Ï†å€¼çš„ç»Ÿè®¡åˆ†æ
                f.write(f"- **æˆåŠŸç»„åˆæ•°**: {len(results)}\n")
                
                entry_counts = [len(r.entries) for r in results]
                if entry_counts:
                    f.write(f"- **éŸ³ç¬¦æ•°é‡èŒƒå›´**: {min(entry_counts)} - {max(entry_counts)}\n")
                    f.write(f"- **å¹³å‡éŸ³ç¬¦æ•°**: {sum(entry_counts)/len(entry_counts):.1f}\n")
                
                f.write("\n")
    
    def _generate_data_exports(self, exploration_results: List[ExplorationResult],
                             evaluations: Dict, classifications: Dict,
                             audio_assessments: Dict, report_dir: Path):
        """ç”Ÿæˆæ•°æ®å¯¼å‡ºæ–‡ä»¶"""
        data_dir = report_dir / "data_exports"
        data_dir.mkdir(exist_ok=True)
        
        # CSVæ ¼å¼çš„å®Œæ•´æ•°æ®
        self._export_complete_data_csv(exploration_results, evaluations, 
                                     classifications, audio_assessments, data_dir)
        
        # JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        self._export_detailed_data_json(exploration_results, evaluations,
                                      classifications, audio_assessments, data_dir)
        
        # éŸ³é˜¶æ–‡ä»¶å¯¼å‡º
        self._export_scale_files(exploration_results, data_dir)
    
    def _export_complete_data_csv(self, exploration_results: List[ExplorationResult],
                                evaluations: Dict, classifications: Dict,
                                audio_assessments: Dict, data_dir: Path):
        """å¯¼å‡ºå®Œæ•´æ•°æ®ä¸ºCSV"""
        csv_path = data_dir / "complete_exploration_data.csv"
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'phi_name', 'phi_value', 'delta_theta_name', 'delta_theta_value', 'f_base',
                'success', 'entry_count', 'min_freq', 'max_freq',
                'avg_interval_cents', 'micro_interval_ratio', 'large_interval_ratio'
            ]
            
            if evaluations:
                headers.extend([
                    'weighted_total_score', 'traditional_compatibility', 'microtonal_potential',
                    'experimental_innovation', 'therapeutic_value', 'harmonic_richness'
                ])
            
            if classifications:
                headers.extend(['primary_category', 'confidence_score'])
            
            if audio_assessments:
                headers.extend(['overall_playability', 'technical_score', 'musical_score', 'audio_recommended'])
            
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®è¡Œ
            for result in exploration_results:
                row = [
                    result.parameters.phi_name,
                    result.parameters.phi_value,
                    result.parameters.delta_theta_name,
                    result.parameters.delta_theta_value,
                    result.parameters.f_base,
                    result.success,
                    len(result.entries) if result.success else 0
                ]
                
                if result.success and result.entries:
                    frequencies = [e.freq for e in result.entries]
                    row.extend([min(frequencies), max(frequencies)])
                else:
                    row.extend([0, 0])
                
                # æ·»åŠ åŸºç¡€æŒ‡æ ‡
                if result.basic_metrics:
                    row.extend([
                        result.basic_metrics.get('avg_interval_cents', 0),
                        result.basic_metrics.get('micro_interval_ratio', 0),
                        result.basic_metrics.get('large_interval_ratio', 0)
                    ])
                else:
                    row.extend([0, 0, 0])
                
                # æ·»åŠ è¯„ä¼°æ•°æ®
                result_key = self._get_result_key(result)
                
                if evaluations and result_key in evaluations:
                    eval_result = evaluations[result_key]
                    row.extend([
                        eval_result.weighted_total_score,
                        eval_result.dimension_scores['traditional_compatibility'].score,
                        eval_result.dimension_scores['microtonal_potential'].score,
                        eval_result.dimension_scores['experimental_innovation'].score,
                        eval_result.dimension_scores['therapeutic_value'].score,
                        eval_result.dimension_scores['harmonic_richness'].score
                    ])
                elif evaluations:
                    row.extend([0] * 6)
                
                # æ·»åŠ åˆ†ç±»æ•°æ®
                if classifications and result_key in classifications:
                    class_result = classifications[result_key]
                    row.extend([
                        class_result.primary_category.value,
                        class_result.confidence_score
                    ])
                elif classifications:
                    row.extend(['', 0])
                
                # æ·»åŠ éŸ³é¢‘æµ‹è¯•æ•°æ®
                if audio_assessments and result_key in audio_assessments:
                    audio_result = audio_assessments[result_key]
                    row.extend([
                        audio_result.overall_playability,
                        audio_result.technical_score,
                        audio_result.musical_score,
                        audio_result.recommended_for_audio
                    ])
                elif audio_assessments:
                    row.extend([0, 0, 0, False])
                
                writer.writerow(row)
    
    def _identify_top_systems(self, successful_results: List[ExplorationResult],
                            evaluations: Dict, count: int = 10) -> List[Dict[str, Any]]:
        """è¯†åˆ«å‰Nåç³»ç»Ÿ"""
        systems = []
        
        for result in successful_results:
            result_key = self._get_result_key(result)
            system = {'exploration_result': result}
            
            if evaluations and result_key in evaluations:
                system['evaluation'] = evaluations[result_key]
            
            systems.append(system)
        
        # æŒ‰è¯„ä¼°å¾—åˆ†æ’åº
        if evaluations:
            systems.sort(key=lambda x: x.get('evaluation', type('obj', (object,), {'weighted_total_score': 0})).weighted_total_score, reverse=True)
        else:
            # å¦‚æœæ²¡æœ‰è¯„ä¼°ï¼ŒæŒ‰éŸ³ç¬¦æ•°é‡æ’åº
            systems.sort(key=lambda x: len(x['exploration_result'].entries), reverse=True)
        
        return systems[:count]
    
    def _get_result_key(self, result: ExplorationResult) -> str:
        """è·å–ç»“æœçš„å”¯ä¸€é”®"""
        params = result.parameters
        return f"{params.phi_name}_{params.delta_theta_name}_{params.f_base}"
    
    def _generate_key_findings(self, successful_results: List[ExplorationResult],
                             evaluations: Dict, classifications: Dict) -> str:
        """ç”Ÿæˆå…³é”®å‘ç°"""
        findings = []
        
        # æˆåŠŸç‡å‘ç°
        total_systems = len(successful_results)
        findings.append(f"æˆåŠŸç”Ÿæˆäº† {total_systems} ä¸ªå¯ç”¨çš„éŸ³å¾‹ç³»ç»Ÿï¼Œæ˜¾ç¤ºPetersenéŸ³å¾‹å…·æœ‰ä¸°å¯Œçš„å‚æ•°ç©ºé—´ã€‚")
        
        # éŸ³ç¬¦æ•°é‡åˆ†å¸ƒ
        entry_counts = [len(r.entries) for r in successful_results]
        if entry_counts:
            avg_entries = sum(entry_counts) / len(entry_counts)
            max_entries = max(entry_counts)
            min_entries = min(entry_counts)
            findings.append(f"éŸ³ç¬¦æ•°é‡åˆ†å¸ƒä» {min_entries} åˆ° {max_entries}ï¼Œå¹³å‡ {avg_entries:.1f} ä¸ªéŸ³ç¬¦ï¼Œå±•ç°äº†ä»ç®€çº¦åˆ°å¤æ‚çš„å¤šæ ·æ€§ã€‚")
        
        # è¯„ä¼°å‘ç°
        if evaluations:
            high_traditional = sum(1 for e in evaluations.values() 
                                 if e.dimension_scores['traditional_compatibility'].score >= 0.7)
            high_experimental = sum(1 for e in evaluations.values() 
                                  if e.dimension_scores['experimental_innovation'].score >= 0.7)
            
            findings.append(f"å‘ç° {high_traditional} ä¸ªä¼ ç»Ÿå…¼å®¹ç³»ç»Ÿå’Œ {high_experimental} ä¸ªé«˜åˆ›æ–°ç³»ç»Ÿï¼Œè¯æ˜PetersenéŸ³å¾‹æ—¢èƒ½ä¼ æ‰¿ä¼ ç»Ÿåˆèƒ½å¼€æ‹“åˆ›æ–°ã€‚")
        
        return "\n".join(f"- {finding}" for finding in findings)
    
    def _generate_application_recommendations(self, top_systems: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆåº”ç”¨å»ºè®®"""
        recommendations = []
        
        if not top_systems:
            return "- æš‚æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆåº”ç”¨å»ºè®®ã€‚"
        
        # ä¼ ç»ŸéŸ³ä¹åº”ç”¨
        traditional_systems = [s for s in top_systems 
                             if 'evaluation' in s and 
                             s['evaluation'].dimension_scores['traditional_compatibility'].score >= 0.7]
        if traditional_systems:
            recommendations.append("**ä¼ ç»ŸéŸ³ä¹åº”ç”¨**: å‘ç°å¤šä¸ªé«˜ä¼ ç»Ÿå…¼å®¹æ€§ç³»ç»Ÿï¼Œé€‚åˆå¤å…¸éŸ³ä¹ç¼–æ›²å’Œå®¤å†…ä¹åˆ›ä½œã€‚")
        
        # å®éªŒéŸ³ä¹åº”ç”¨
        experimental_systems = [s for s in top_systems 
                              if 'evaluation' in s and 
                              s['evaluation'].dimension_scores['experimental_innovation'].score >= 0.7]
        if experimental_systems:
            recommendations.append("**å®éªŒéŸ³ä¹åº”ç”¨**: è¯†åˆ«å‡ºåˆ›æ–°æ€§ç³»ç»Ÿï¼Œé€‚åˆç°ä»£ä½œæ›²å’Œç”µå­éŸ³ä¹åˆ¶ä½œã€‚")
        
        # æ•™è‚²åº”ç”¨
        moderate_systems = [s for s in top_systems 
                          if len(s['exploration_result'].entries) >= 8 and len(s['exploration_result'].entries) <= 20]
        if moderate_systems:
            recommendations.append("**æ•™è‚²åº”ç”¨**: ä¸­ç­‰å¤æ‚åº¦ç³»ç»Ÿé€‚åˆéŸ³ä¹æ•™è‚²å’Œå¬åŠ›è®­ç»ƒã€‚")
        
        return "\n".join(f"- {rec}" for rec in recommendations)
    
    def _generate_main_report_index(self, report_dir: Path, report_name: str) -> Path:
        """ç”Ÿæˆä¸»æŠ¥å‘Šç´¢å¼•"""
        index_path = report_dir / "README.md"
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(f"# {report_name} - PetersenéŸ³å¾‹ç³»ç»Ÿæ¢ç´¢æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“‹ æŠ¥å‘Šç»“æ„\n\n")
            f.write("### æ ¸å¿ƒæŠ¥å‘Š\n")
            f.write("- [æ‰§è¡Œæ‘˜è¦](executive_summary.md) - å…³é”®å‘ç°å’Œæ¨è\n")
            f.write("- [è¯¦ç»†åˆ†æ](detailed_analysis/) - æ·±åº¦å‚æ•°åˆ†æ\n")
            f.write("- [åº”ç”¨å»ºè®®](recommendations.md) - å®ç”¨æŒ‡å¯¼\n\n")
            
            f.write("### æ•°æ®æ–‡ä»¶\n")
            f.write("- [æ•°æ®å¯¼å‡º](data_exports/) - CSVå’ŒJSONæ ¼å¼æ•°æ®\n")
            f.write("- [éŸ³é˜¶æ–‡ä»¶](data_exports/scale_files/) - å¯å¯¼å…¥çš„éŸ³é˜¶æ–‡ä»¶\n\n")
            
            f.write("### æŠ€æœ¯æ–‡æ¡£\n")
            f.write("- [æ–¹æ³•è®º](methodology.md) - è¯„ä¼°æ–¹æ³•å’Œæ ‡å‡†\n")
            f.write("- [å‚æ•°è¯´æ˜](parameter_reference.md) - Ï†å’ŒÎ´Î¸é¢„è®¾è¯¦è§£\n\n")
            
            f.write("## ğŸ¯ å¿«é€Ÿå¯¼èˆª\n\n")
            f.write("- **å¯»æ‰¾æœ€ä½³ç³»ç»Ÿ**: æŸ¥çœ‹[æ‰§è¡Œæ‘˜è¦](executive_summary.md)çš„ä¼˜ç§€ç³»ç»Ÿæ¨è\n")
            f.write("- **äº†è§£å‚æ•°å½±å“**: æŸ¥çœ‹[è¯¦ç»†åˆ†æ](detailed_analysis/)\n")
            f.write("- **è·å–ä½¿ç”¨å»ºè®®**: æŸ¥çœ‹[åº”ç”¨å»ºè®®](recommendations.md)\n")
            f.write("- **ä¸‹è½½æ•°æ®**: æŸ¥çœ‹[æ•°æ®å¯¼å‡º](data_exports/)\n")
        
        return index_path
    
    # å…¶ä»–è¾…åŠ©æ–¹æ³•...
    def _export_detailed_data_json(self, exploration_results, evaluations, 
                                 classifications, audio_assessments, data_dir):
        """å¯¼å‡ºè¯¦ç»†JSONæ•°æ®"""
        # å®ç°JSONå¯¼å‡ºé€»è¾‘
        pass
    
    def _export_scale_files(self, exploration_results, data_dir):
        """å¯¼å‡ºéŸ³é˜¶æ–‡ä»¶"""
        # å®ç°éŸ³é˜¶æ–‡ä»¶å¯¼å‡ºé€»è¾‘
        pass
    
    def _analyze_f_base_effects(self, exploration_results, evaluations, analysis_dir):
        """åˆ†æF_baseæ•ˆåº”"""
        # å®ç°F_baseåˆ†æé€»è¾‘
        pass
    
    def _analyze_musical_characteristics(self, evaluations, analysis_dir):
        """åˆ†æéŸ³ä¹ç‰¹æ€§"""
        # å®ç°éŸ³ä¹ç‰¹æ€§åˆ†æé€»è¾‘
        pass
    
    def _analyze_audio_performance(self, audio_assessments, analysis_dir):
        """åˆ†æéŸ³é¢‘æ€§èƒ½"""
        # å®ç°éŸ³é¢‘æ€§èƒ½åˆ†æé€»è¾‘
        pass